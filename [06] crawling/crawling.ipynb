{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling\n",
    "\n",
    "220421\n",
    "\n",
    "크롤링 ppt \n",
    "https://docs.google.com/presentation/d/1aRs0sEbfAakwDV8Xd1l-pRuUgYi9XQLVm4jAd29kV7Q/edit#slide=id.g8345f8eaca_4_244\n",
    "\n",
    "코랩\n",
    "https://colab.research.google.com/drive/1UxfgsJF-Htu3miacTytnlW7lpGR90xY8?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "4/20\n",
    "\n",
    "https://drive.google.com/drive/folders/1969-SfVsB9Sxt-KYXRu7-dxJhngiXEk5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "4/25\n",
    "\n",
    "https://colab.research.google.com/drive/1UxfgsJF-Htu3miacTytnlW7lpGR90xY8?usp=sharing\n",
    "\n",
    "https://docs.google.com/presentation/d/1aRs0sEbfAakwDV8Xd1l-pRuUgYi9XQLVm4jAd29kV7Q/edit#slide=id.g8345f8eaca_4_192\n",
    "\n",
    "https://docs.google.com/presentation/d/1n9pHEp9sUmdoD95zuMUj1cnktNIF-fM-mAmOnoy4XSQ/edit#slide=id.p\n",
    "\n",
    "http://www.pythonchallenge.com/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "4/26\n",
    "\n",
    "https://realpython.com/brython-python-in-browser/\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 크롤러"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### library requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://info.cern.ch\"\n",
    "page = requests.get(url)\n",
    "\n",
    "print(page.text)\n",
    "print(page.status_code)\n",
    "print(page.headers)\n",
    "\n",
    "with open(\"cern.html\", 'w') as f:\n",
    "    f.write(page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이미지 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://k.kakaocdn.net/dn/bzEOMk/btra1YZ7BFG/PGmK1cObPiemmYifhzCkak/img.jpg\"\n",
    "res = requests.get(url)\n",
    "\n",
    "# print(res.content)\n",
    "with open(\"spiderman.jpg\", 'wb') as f:\n",
    "    f.write(res.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://comic.naver.com/webtoon/detail?titleId=597447&no=230&weekday=sat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    \"referer\": \"https://comic.naver.com/webtoon/detail?titleId=597447&no=230&weekday=sat\"\n",
    "}\n",
    "\n",
    "url = \"https://image-comic.pstatic.net/webtoon/597447/230/20180420172034_974f4dafcbf6098e4e04a19baae325ac_IMAG01_1.jpg\"\n",
    "res = requests.get(url, headers=headers)\n",
    "\n",
    "print(res.status_code)\n",
    "with open(\"naver.jpg\", 'wb') as f:\n",
    "    f.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## json \n",
    "- csv: 사람이 데이터를 표의 형태로 보기 위해서 만든 표준 포멧\n",
    "- json: 컴퓨터끼리 데이터를 쉽게 주고받기 위해서 만든 포맷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.github.com/\"\n",
    "res = requests.get(url)\n",
    "\n",
    "print(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_obj = json.loads(res.text)\n",
    "\n",
    "# json_obj = res.json()\n",
    "print(json_obj)\n",
    "print(json_obj['current_user_url'])\n",
    "print(json_obj['current_user_authorizations_html_url'])\n",
    "\n",
    "for k, v in json_obj.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "res = requests.get(\"https://api.github.com/users\")\n",
    "users = res.json()\n",
    "\n",
    "for u in users:\n",
    "    print(u['login'], u['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "res = requests.get(\"https://api.github.com/users\")\n",
    "users = res.json()\n",
    "\n",
    "df = pd.DataFrame(users)\n",
    "df.to_csv('user.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"https://api.github.com/users\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{\"a\": 1, \"b\": 2, \"c\": 3}, {\"a\": 2, \"b\": 3, \"c\": 4}]\n",
    "import json\n",
    "json.dumps(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup (아름다운 국물)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "\n",
    "res = requests.get(\"http://info.cern.ch\")\n",
    "print(res.text)\n",
    "\n",
    "soup = bs4.BeautifulSoup(res.text, \"html.parser\")\n",
    "print(soup.title)\n",
    "print(soup.ul)\n",
    "print(soup.a)\n",
    "print(soup.ul.li)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 링크를 찾아서 자동으로 돌아다니는 로봇. 크롤러 만들기 \n",
    "1. **a 태그를 모두 찾는다.** \n",
    "1. **태그에서 링크를 추출**하여 url 리스트를 만든다. \n",
    "1. url 리스트에서 링크를 하나 꺼내서 requests 결과를 받는다. \n",
    "1. 받은 결과를 soup에 넣고 a 태그를 찾는다. (반복)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 내가 원하는 태그를 찾는 방법\n",
    "  - soup.tag\n",
    "  - soup.select('tag')\n",
    "- 태그에서 정보를 추출하는 방법\n",
    "  - soup['속성'] - attribute 값을 꺼내기\n",
    "  - soup.get_text() - 태그가 감싸고 있는 텍스트 꺼내기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "\n",
    "res = requests.get(\"http://info.cern.ch\")\n",
    "soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 가까운 태그 찾기 \n",
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원하는 태그를 모두 찾기\n",
    "tags = soup.select(\"a\")\n",
    "for t in tags:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원하는 태그를 모두 찾기\n",
    "tags = soup.select(\"a\")\n",
    "for t in tags:\n",
    "    print(t)\n",
    "    # 원하는 태그의 속성값 꺼내기\n",
    "    print(t['href'])\n",
    "    # 원하는 태그의 텍스트 꺼내기\n",
    "    print(t.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_text는 태그를 모두 지운다고 생각하면 됩니다. \n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 링크를 꺼낸다.\n",
    "links = []\n",
    "\n",
    "tags = soup.select(\"a\")\n",
    "for t in tags:\n",
    "    links.append(t['href'])\n",
    "\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 링크를 꺼낸다.\n",
    "links = []\n",
    "\n",
    "tags = soup.select(\"a\")\n",
    "for t in tags:\n",
    "    links.append(t['href'])\n",
    "\n",
    "while links:\n",
    "    link = links.pop(0)\n",
    "    print(link)\n",
    "    print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4]\n",
    "print(a.pop())\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "\n",
    "def get_links(url):\n",
    "    res = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "    links = []\n",
    "    for t in soup.select('a'):\n",
    "        if t.get(\"href\") is not None and t.get(\"href\")[:4] == 'http':\n",
    "            links.append(t['href'])\n",
    "    # links = [t['href'] for t in soup.select('a') if t.get(\"href\") is not None and t.get(\"href\")[:4] == 'http']\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = get_links('http://info.cern.ch')\n",
    "print(len(links)) # 4개 \n",
    "\n",
    "while links:\n",
    "    url = links.pop(0)\n",
    "    print(url)\n",
    "    links += get_links(url)\n",
    "    print(len(links), links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4] + [4, 5, 6]\n",
    "print(a)\n",
    "a = list(set(a)) # 중복제거\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스크래핑: 영화 박스오피스 리스트 추출하기\n",
    "- https://movie.daum.net/ranking/boxoffice/weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "\n",
    "url = \"https://movie.daum.net/ranking/boxoffice/weekly\"\n",
    "res = requests.get(url)\n",
    "soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = soup.select(\"div.item_poster\")\n",
    "print(len(tags))\n",
    "tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"제목\", tags[0].select(\"strong.tit_item\")[0].get_text())\n",
    "print(\"상세링크\", tags[0].select(\"a.link_txt\")[0]['href'])\n",
    "print(\"설명\", tags[0].select(\"a.link_story\")[0].get_text().strip())\n",
    "print(\"포스터\", tags[0].select(\"img.img_thumb\")[0]['src'])\n",
    "print(\"관람가능연령\", tags[0].select(\"span.ico_see\")[0].get_text())\n",
    "print(\"개봉일\", tags[0].select(\"span.info_txt\")[0].get_text())\n",
    "print(\"관객수\", tags[0].select(\"span.info_txt\")[1].get_text())\n",
    "print(\"순위\", tags[0].select(\"span.rank_num\")[0].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제목, 설명만 꺼내서 리스트로 만들어보자. \n",
    "# movies = [{'제목': '영화제목', '설명': '영화설명'},\n",
    "#           {'제목': '영화제목', '설명': '영화설명'}]\n",
    "movies = []\n",
    "\n",
    "for t in tags:\n",
    "    movies.append({\n",
    "        '제목': t.select(\"strong.tit_item\")[0].get_text(),\n",
    "        '상세링크': t.select(\"a.link_txt\")[0]['href'],\n",
    "        '설명': t.select(\"a.link_story\")[0].get_text().strip(),\n",
    "        '포스터': t.select(\"img.img_thumb\")[0]['src'],\n",
    "        '관람가능연령': t.select(\"span.ico_see\")[0].get_text(),\n",
    "        '개봉일': t.select(\"span.info_txt\")[0].get_text(),\n",
    "        '관객수': t.select(\"span.info_txt\")[1].get_text(),\n",
    "        '순위': t.select(\"span.rank_num\")[0].get_text()\n",
    "    })\n",
    "\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(movies)\n",
    "df.to_csv('movies.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "con = sqlite3.connect(\"daum_movies.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql('boxoffice', con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = con.cursor()\n",
    "data = cur.execute(\"select * from boxoffice where 제목 = '앵커'\")\n",
    "for row in data.fetchall():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "\n",
    "url = \"https://movie.daum.net/ranking/boxoffice/weekly\"\n",
    "res = requests.get(url)\n",
    "soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "tags = soup.select(\"div.item_poster\")\n",
    "tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제목, 설명만 꺼내서 리스트로 만들어보자. \n",
    "# movies = [{'제목': '영화제목', '설명': '영화설명'},\n",
    "#           {'제목': '영화제목', '설명': '영화설명'}]\n",
    "movies = []\n",
    "\n",
    "for t in tags:\n",
    "    movies.append({\n",
    "        '제목': t.select(\"strong.tit_item\")[0].get_text(),\n",
    "        '상세링크': t.select(\"a.link_txt\")[0]['href'],\n",
    "        '설명': t.select(\"a.link_story\")[0].get_text().strip(),\n",
    "        '포스터': t.select(\"img.img_thumb\")[0]['src'],\n",
    "        '관람가능연령': t.select(\"span.ico_see\")[0].get_text(),\n",
    "        '개봉일': t.select(\"span.info_txt\")[0].get_text(),\n",
    "        '관객수': t.select(\"span.info_txt\")[1].get_text(),\n",
    "        '순위': t.select(\"span.rank_num\")[0].get_text()\n",
    "    })\n",
    "\n",
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2020년 1월 부터 지금까지의 영화 목록 뽑기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['20200101', '20200107', '20200114']\n",
    "for d in dates:\n",
    "    url = f\"https://movie.daum.net/ranking/boxoffice/weekly?date={d}\"\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "date = datetime.date(2020, 1, 1)\n",
    "print(date)\n",
    "date = date + datetime.timedelta(weeks=1)\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dates = pd.date_range('2020-01-01', '2022-04-22', freq=\"W\")\n",
    "for d in dates:\n",
    "    url = f\"https://movie.daum.net/ranking/boxoffice/weekly?date={d.strftime('%Y%m%d')}\"\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제목, 설명만 꺼내서 리스트로 만들어보자. \n",
    "# movies = [{'제목': '영화제목', '설명': '영화설명'},\n",
    "#           {'제목': '영화제목', '설명': '영화설명'}]\n",
    "import requests\n",
    "import bs4\n",
    "\n",
    "def get_boxoffice(url, date=None):\n",
    "    print(url)\n",
    "    res = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "    movies = []\n",
    "    for t in soup.select(\"div.item_poster\"):\n",
    "        movies.append({\n",
    "            '제목': t.select(\"strong.tit_item\")[0].get_text(),\n",
    "            '상세링크': t.select(\"a.link_txt\")[0]['href'],\n",
    "            '설명': t.select(\"a.link_story\")[0].get_text().strip(),\n",
    "            '포스터': t.select(\"img.img_thumb\")[0]['src'] if t.select(\"img.img_thumb\") else None,\n",
    "            '관람가능연령': t.select(\"span.ico_see\")[0].get_text() if t.select(\"span.ico_see\") else None,\n",
    "            '개봉일': t.select(\"span.info_txt\")[0].get_text() if len(t.select(\"span.info_txt\")) == 2 else None,\n",
    "            '관객수': t.select(\"span.info_txt\")[1].get_text() if len(t.select(\"span.info_txt\")) == 2 else None,\n",
    "            '순위': t.select(\"span.rank_num\")[0].get_text(),\n",
    "            '주간': date\n",
    "        })\n",
    "    \n",
    "    return movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dates = pd.date_range('2020-01-01', '2022-04-22', freq=\"W\")\n",
    "\n",
    "movies = []\n",
    "for d in dates:\n",
    "    url = f\"https://movie.daum.net/ranking/boxoffice/weekly?date={d.strftime('%Y%m%d')}\"\n",
    "    movies += get_boxoffice(url, d)\n",
    "\n",
    "print(len(movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(movies)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'authority': 'image-comic.pstatic.net',\n",
    "    'accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',\n",
    "    'accept-language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "    'cache-control': 'no-cache',\n",
    "    'pragma': 'no-cache',\n",
    "    'referer': 'https://comic.naver.com/webtoon/detail?titleId=597447&no=230&weekday=sat',\n",
    "    'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"100\", \"Google Chrome\";v=\"100\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"macOS\"',\n",
    "    'sec-fetch-dest': 'image',\n",
    "    'sec-fetch-mode': 'no-cors',\n",
    "    'sec-fetch-site': 'cross-site',\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36',\n",
    "}\n",
    "\n",
    "response = requests.get('https://image-comic.pstatic.net/webtoon/597447/230/20180420172034_974f4dafcbf6098e4e04a19baae325ac_IMAG01_2.jpg', headers=headers)\n",
    "\n",
    "with open('naver2.jpg', 'wb') as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://comic.naver.com/webtoon/detail?titleId=597447&no=230&weekday=sat\"\n",
    "res = requests.get(url)\n",
    "soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "tags = soup.select(\"div.wt_viewer img\")\n",
    "for t in tags: \n",
    "    print(t['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'authority': 'image-comic.pstatic.net',\n",
    "    'accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',\n",
    "    'accept-language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "    'cache-control': 'no-cache',\n",
    "    'pragma': 'no-cache',\n",
    "    'referer': 'https://comic.naver.com/webtoon/detail?titleId=597447&no=230&weekday=sat',\n",
    "    'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"100\", \"Google Chrome\";v=\"100\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"macOS\"',\n",
    "    'sec-fetch-dest': 'image',\n",
    "    'sec-fetch-mode': 'no-cors',\n",
    "    'sec-fetch-site': 'cross-site',\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36',\n",
    "}\n",
    "\n",
    "for i, t in enumerate(tags): \n",
    "    response = requests.get(t['src'], headers=headers)\n",
    "    with open(f'naver{i}.jpg', 'wb') as f:\n",
    "        f.write(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in enumerate(['a', 'b', 'c']):\n",
    "    print(i, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방탈출\n",
    "http://www.pythonchallenge.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 ** 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ord('a'))\n",
    "print(chr(97))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"g fmnc wms bgblr rpylqjyrc gr zw fylb. rfyrq ufyr amknsrcpq ypc dmp. bmgle gr gl zw fylb gq glcddgagclr ylb rfyr'q ufw rfgq rcvr gq qm jmle. sqgle qrpgle.kyicrpylq() gq pcamkkclbcb. lmu ynnjw ml rfc spj.\"\n",
    "\n",
    "source = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "target = \"cdefghijklmnopqrstuvwxyzab\"\n",
    "\n",
    "for a in txt:\n",
    "    if a in source:\n",
    "        print(target[list(source).index(a)], end='')\n",
    "    else:\n",
    "        print(a, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'map'\n",
    "table = txt.maketrans(source, target)\n",
    "txt.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"a,b,c<!--!@#$!#$TEAGFA@!@$#RADSF-->\"\n",
    "txt.split('<!--')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'http://www.pythonchallenge.com/pc/def/ocr.html'\n",
    "\n",
    "res = requests.get(url)\n",
    "mess = res.text.split('<!--')[-1]\n",
    "mess = mess.replace('\\n', '')\n",
    "print(len(mess), mess[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex = re.compile('[\\W_]')\n",
    "regex.sub('', mess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special = '!@#$%^&U*()_+[]{}\\n'\n",
    "for s in special:\n",
    "    mess = mess.replace(s, '')\n",
    "mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in mess:\n",
    "    if m.isalnum():\n",
    "        print(m, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'http://www.pythonchallenge.com/pc/def/equality.html'\n",
    "\n",
    "res = requests.get(url)\n",
    "mess = res.text.split('<!--')[-1]\n",
    "mess = mess.replace('\\n', '')\n",
    "print(len(mess), mess[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"\"\"RegExr was created by gskinner.com, and is proudly hosted by Media Temple.\n",
    "Edit the Expression & Text to see matches. Roll over matches or the expression for details. \n",
    "PCRE & JavaScript flavors of RegEx are supported. Validate your expression with Tests mode.\n",
    "The side bar includes a Cheatsheet, full Reference, and Help. \n",
    "You can also Save & Share with the Community, and view patterns you create or favorite in My Patterns.\n",
    "Explore results with the Tools below. Replace & List output custom results. \n",
    "Details lists capture groups. Explain describes your expression in plain English.\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "regex = re.compile('[A-Z]\\w+')\n",
    "result = regex.findall(test)\n",
    "result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex = re.compile('[a-z][A-Z][A-Z][A-Z][a-z][A-Z][A-Z][A-Z][a-z]')\n",
    "result = regex.findall(mess)\n",
    "print(result)\n",
    "print(''.join([r[4] for r in result]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex = re.compile('[a-z][A-Z][A-Z][A-Z]([a-z])[A-Z][A-Z][A-Z][a-z]')\n",
    "result = regex.findall(mess)\n",
    "print(result)\n",
    "print(''.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"http://www.pythonchallenge.com/pc/def/linkedlist.php?nothing=12345\"\n",
    "\n",
    "res = requests.get(url)\n",
    "print(res.text)\n",
    "next = res.text.split()[-1]\n",
    "\n",
    "next = \"8022\"\n",
    "for i in range(500):\n",
    "    url = f\"http://www.pythonchallenge.com/pc/def/linkedlist.php?nothing={next}\"\n",
    "    res = requests.get(url)\n",
    "    print(res.text)\n",
    "    next = res.text.split()[-1]\n",
    "    if not next.isdigit():\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Json file 읽고 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.github.com/\"\n",
    "res = requests.get(url)\n",
    "\n",
    "with open(\"github.json\", \"w\") as f:\n",
    "    f.write(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 읽기\n",
    "import json\n",
    "\n",
    "with open(\"github.json\", \"r\") as f:\n",
    "    json_str = f.read()\n",
    "\n",
    "json_obj = json.loads(json_str)\n",
    "print(json_obj)\n",
    "print(json_obj['current_user_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쓰기\n",
    "import json\n",
    "\n",
    "test = {\"a\": \"hello\", \"b\": \"test\"}\n",
    "test_json = json.dumps(test)\n",
    "print(test_json)\n",
    "\n",
    "with open(\"test.json\", \"w\") as f:\n",
    "    f.write(test_json)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webai",
   "language": "python",
   "name": "webai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
